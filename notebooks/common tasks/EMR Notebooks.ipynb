{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to utilize Spark in a cluster environment You need to first spin up a cluster and then attach a notebook to the cluster instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create EMR Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_NAME=\"jvs-fraud\"\n",
    "LOG_URI=f\"s3://aws-logs-119657064844-us-east-1/\"\n",
    "RELEASE_LABEL=\"emr-5.32.0\" # Max version for use with Notebooks\n",
    "NUM_CORE_NODES=2\n",
    "NUM_INSTANCES=1 + NUM_CORE_NODES # 1 Master + N slaves\n",
    "\n",
    "# Must use AMD EPYC instances ie m5a.*\n",
    "INSTANCE_TYPE=\"m5a.xlarge\"  # $0.172/hr 4core-16GB RAM\n",
    "\n",
    "EC2_KEY=\"jvs_ec2_key\"\n",
    "EC2_SUBNET=\"subnet-171c8c4b\"\n",
    "REGION=\"us-east-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create EC2 Key Pair and PEM File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Go to [Amazon EC2 console](https://console.aws.amazon.com/ec2/home)\n",
    "2. click Key Pairs\n",
    "3. Create Key Pairs\n",
    "4. Make a name\n",
    "5. Save PEM file locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using AWS CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the CLI command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    aws emr create-cluster \\\n",
      "    --release-label emr-5.32.0 \\\n",
      "    --applications Name=Spark \\\n",
      "    --instance-type m5.xlarge \\\n",
      "    --instance-count 3 \\\n",
      "    --name jvs-fraud \\\n",
      "    --log-uri s3://aws-logs-jvs-fraud/elasticmapreduce/ \\\n",
      "    --use-default-roles \\\n",
      "    --ec2-attributes KeyName=jvs_ec2_key \\\n",
      "    --region us-east-1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "make_cluster_cmd = f\"\"\"\n",
    "    aws emr create-cluster \\\\\n",
    "    --release-label {RELEASE_LABEL} \\\\\n",
    "    --applications Name=Spark \\\\\n",
    "    --instance-type {INSTANCE_TYPE} \\\\\n",
    "    --instance-count {NUM_INSTANCES} \\\\\n",
    "    --name {CLUSTER_NAME} \\\\\n",
    "    --log-uri s3://aws-logs-{CLUSTER_NAME}/elasticmapreduce/ \\\\\n",
    "    --use-default-roles \\\\\n",
    "    --ec2-attributes KeyName={EC2_KEY},SubnetId={EC2_SUBNET} \\\\\n",
    "    --region {REGION}\n",
    "\"\"\"\n",
    "print(make_cluster_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the command and **create the cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{make_cluster_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the Cluster ID above and input to the cell below to run the command and **terminate the cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_ID=\"j-3GTMZ6TB4TEEM\"\n",
    "#!aws emr terminate-clusters --cluster-ids {CLUSTER_ID} --region us-east-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USING Python - boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we want our notebook data to persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME=\"jvs-fraud-research\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3', REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the bucket if doesn't exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BUCKET_NAME not in [val['Name'] for val in s3.list_buckets()['Buckets']]:\n",
    "    s3.create_bucket(Bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/26314316/how-to-launch-and-configure-an-emr-cluster-using-boto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMR ID: j-2X4G7C1R1F89B\n"
     ]
    }
   ],
   "source": [
    "emr = boto3.client('emr', region_name=REGION)\n",
    "\n",
    "response = emr.run_job_flow(\n",
    "    Name=CLUSTER_NAME,\n",
    "    LogUri=LOG_URI,\n",
    "    ReleaseLabel=RELEASE_LABEL,\n",
    "    Instances={\n",
    "        'MasterInstanceType': INSTANCE_TYPE,\n",
    "        'SlaveInstanceType': INSTANCE_TYPE,\n",
    "        'InstanceCount': NUM_INSTANCES,\n",
    "        'KeepJobFlowAliveWhenNoSteps': True,\n",
    "        'TerminationProtected': False,\n",
    "        'Ec2KeyName': EC2_KEY,\n",
    "        'Ec2SubnetId': EC2_SUBNET\n",
    "    },\n",
    "    Applications=[\n",
    "        {'Name': 'Spark'},\n",
    "        {'Name': 'Hadoop'},\n",
    "        {'Name': 'Livy'}\n",
    "    ],\n",
    "    VisibleToAllUsers=True,\n",
    "    JobFlowRole='EMR_EC2_DefaultRole',\n",
    "    ServiceRole='EMR_DefaultRole'\n",
    "    \n",
    ")\n",
    "print(f\"EMR ID: {response['JobFlowId']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the EMR is stood up you can go to the [EMR Notebooks Page](https://console.aws.amazon.com/elasticmapreduce/home?region=us-east-1#notebooks-list:) To create (or open) a notebook to work from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'b02a5a98-824c-4290-9915-2e1bbdd7431d',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'b02a5a98-824c-4290-9915-2e1bbdd7431d',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Fri, 18 Dec 2020 22:59:44 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emr.terminate_job_flows(JobFlowIds=[response['JobFlowId']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/getting-started-with-pyspark-on-amazon-emr-c85154b6b921"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
